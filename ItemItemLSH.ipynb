{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f339988-b5d8-4b0c-915b-869ea793718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pedro Cruz\\AppData\\Local\\Temp\\ipykernel_22428\\3230840595.py:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  os.environ[\"SPARK_HOME\"] = \"C:\\spark\\spark-3.5.5-bin-hadoop3\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "#Run with python 3.8\n",
    "\n",
    "#Comentar as três linhas abaixo caso esteja a usar o portátil\n",
    "spark_home = os.path.join(os.environ[\"CONDA_PREFIX\"], \"lib/python3.8/site-packages/pyspark\")\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\spark\\spark-3.5.5-bin-hadoop3\"\n",
    "os.environ[\"PATH\"] = f\"{spark_home}/bin:{os.environ['PATH']}\"\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable \n",
    "#without this pyspark doesnt work on my computer\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, MinHashLSH, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4df2d61-76db-4146-9df4-e9cf941a2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Assignment3B\").config(\"spark.driver.maxResultSize\", \"6g\").config(\"spark.executor.memoryOverhead\", \"6g\").config(\"spark.driver.memoryOverhead\", \"6g\").config(\"spark.memory.fraction\", \"0.6\").config(\"spark.sql.autoBroadcastJoinThreshold\", -1).config(\"spark.memory.storageFraction\", \"0.3\").config(\"spark.sql.shuffle.spill\", \"true\").config(\"spark.driver.memory\", \"16g\").config(\"spark.executor.memory\", \"6g\").config(\"spark.sql.shuffle.partitions\", \"150\").config(\"spark.memory.offHeap.enabled\", \"true\").config(\"spark.memory.offHeap.size\", \"4g\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3ae96-3c67-46b1-98ba-6ee65ef05599",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b8ff6-3c55-42ca-892e-f520778f694b",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "#https://grouplens.org/datasets/movielens/\n",
    "\n",
    "#Using dataset recommended for education and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67881a74-a985-477a-a997-84546eefe8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.option(\"header\", True).csv(\"ml-latest/ratings.csv\")\n",
    "data = data.drop(\"timestamp\")\n",
    "#limiting to 100000 samples\n",
    "data = data.limit(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11bcce-fea3-4dd5-900f-d6ea1678fe8a",
   "metadata": {},
   "source": [
    "As the data format is UserID, MovieID, Rating. I will transform it into movieID, userId-rating as we are doing item-item CF this way i can create discreet sets of data, and apply a LSH to check users that rated the same for the same movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5afd9848-a4b3-4d15-bca3-a21d59c62cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------+\n",
      "|userId|movieId|rating|userId-rating|\n",
      "+------+-------+------+-------------+\n",
      "|     1|      1|   4.0|        1-4.0|\n",
      "|     1|   1036|   5.0|        1-5.0|\n",
      "|     1|   1049|   3.0|        1-3.0|\n",
      "|     1|   1066|   4.0|        1-4.0|\n",
      "|     1|    110|   4.0|        1-4.0|\n",
      "|     1|   1196|   3.5|        1-3.5|\n",
      "|     1|   1210|   4.5|        1-4.5|\n",
      "|     1|   1291|   5.0|        1-5.0|\n",
      "|     1|   1293|   2.0|        1-2.0|\n",
      "|     1|   1376|   3.0|        1-3.0|\n",
      "|     1|   1396|   3.0|        1-3.0|\n",
      "|     1|   1537|   4.0|        1-4.0|\n",
      "|     1|    158|   4.0|        1-4.0|\n",
      "|     1|   1909|   3.0|        1-3.0|\n",
      "|     1|   1959|   4.0|        1-4.0|\n",
      "|     1|   1960|   4.0|        1-4.0|\n",
      "|     1|   2028|   5.0|        1-5.0|\n",
      "|     1|   2085|   3.5|        1-3.5|\n",
      "|     1|   2116|   4.0|        1-4.0|\n",
      "|     1|   2336|   3.5|        1-3.5|\n",
      "+------+-------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"userId-rating\", F.concat(F.col(\"userId\"), F.lit(\"-\"), F.col(\"rating\")))\n",
    "#Spliting the data 90% to implement and 10% for validation, seed for reproducibility\n",
    "data, data_val = data.randomSplit([0.9, 0.1], seed=42)\n",
    "#Will preprocess the data and validation data exactly the same way\n",
    "data.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e730915-2bae-420d-a049-0d1259350de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|            Id-Rates|\n",
      "+-------+--------------------+\n",
      "|      1|[1000-3.5, 1001-5...|\n",
      "|     10|[1001-4.0, 1003-3...|\n",
      "|    100|[1109-5.0, 1324-3...|\n",
      "|   1000|[2401-4.0, 3078-4...|\n",
      "| 100008|[4074-3.0, 4880-3...|\n",
      "| 100017|[6001-3.5, 7629-3.5]|\n",
      "| 100032|          [9401-4.5]|\n",
      "| 100034|          [2651-2.5]|\n",
      "| 100038|          [9068-3.5]|\n",
      "| 100044|[2589-4.0, 305-4....|\n",
      "| 100046|[3469-3.0, 3917-3...|\n",
      "| 100054|          [8833-4.0]|\n",
      "| 100058|           [305-2.0]|\n",
      "| 100062|          [8204-3.0]|\n",
      "| 100081|          [8833-2.0]|\n",
      "| 100083|[1117-4.0, 1195-2...|\n",
      "| 100087|           [389-3.0]|\n",
      "| 100091|          [8833-2.5]|\n",
      "| 100106|[4202-4.5, 4249-5...|\n",
      "| 100108|[2004-3.5, 2172-1...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We will now trasform the movieId-ratings into the following format userId, [list of movieId-ratings]\n",
    "feats = data.groupBy(\"movieId\").agg(F.collect_list(\"userId-rating\").alias(\"Id-Rates\"))\n",
    "feats.limit(20).show()\n",
    "#feats_val = data_val.groupBy(\"movieId\").agg(F.collect_list(\"userId-rating\").alias(\"Id-Rates\"))\n",
    "#Now the format is ideal to construct the equivalent to a characteristics matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99751d38-311e-4d61-84f1-7bc15869a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer pretty much constructs the characteristic matrix, if there is too much data will substitute by HashingTF\n",
    "#Countvectorizer substituted by HashingTF\n",
    "htf = HashingTF(inputCol=\"Id-Rates\", outputCol=\"vectorized_rates\", numFeatures=2**16)\n",
    "feat_vector = htf.transform(feats) #vector.transform caso use o countvectorizer\n",
    "#feat_vector_val = htf.transform(feats_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb084d0b-dd78-486c-9588-baca8ef2493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have our characteristics matrix interpreted as a list of vectors for ALL users\n",
    "#The data format here is a list of [Indexes of Existing movieIc-rating on a user]\n",
    "#As an example we have | (userId)1|(movieId-ratings)[1-4.0, 1036-5.0,...|(Vectorized)(28932,[8,9,23,36...|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124effb6-0530-49cc-a194-9c0a588ffcec",
   "metadata": {},
   "source": [
    "### LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd6903-2400-41e3-b6c8-1943267133d7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "We can now proceed to minHashing\n",
    "Probability of candidate is 1-(1-s^k)^b, where k is the number of hashed rows and b is number of bands\n",
    "and s is the threshold similarity\n",
    "I will choose s = 0.5 because we are looking into identical behaviour in movie ratings\n",
    "If User1 has a similarity rating of 0.5 with user2 then we can recommend movies for both users\n",
    "As i dont know how many hash funtions to use ill just use the default value for pyspark's MinHashLSH \n",
    "(cant use the default value as it is not working too much memory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80cb1f03-6bc4-456b-8491-f3bf3a5be93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|movieId|            Id-Rates|    vectorized_rates|        hashed_rates|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|      1|[1000-3.5, 1001-5...|(65536,[65,71,78,...|[[431617.0], [606...|\n",
      "|     10|[1001-4.0, 1003-3...|(65536,[99,308,35...|[[1380108.0], [18...|\n",
      "|    100|[1109-5.0, 1324-3...|(65536,[681,717,2...|[[1.3880476E7], [...|\n",
      "|   1000|[2401-4.0, 3078-4...|(65536,[24495,302...|[[1.214088191E9],...|\n",
      "| 100008|[4074-3.0, 4880-3...|(65536,[3865,3283...|[[7.62221094E8], ...|\n",
      "| 100017|[6001-3.5, 7629-3.5]|(65536,[26398,405...|[[1.136325078E9],...|\n",
      "| 100032|          [9401-4.5]|(65536,[34386],[1...|[[1.69828445E9], ...|\n",
      "| 100034|          [2651-2.5]|(65536,[53851],[1...|[[5.5908619E8], [...|\n",
      "| 100038|          [9068-3.5]|(65536,[31590],[1...|[[8.99163005E8], ...|\n",
      "| 100044|[2589-4.0, 305-4....|(65536,[2549,4009...|[[5.204915E8], [2...|\n",
      "| 100046|[3469-3.0, 3917-3...|(65536,[25537,315...|[[5.606449E8], [8...|\n",
      "| 100054|          [8833-4.0]|(65536,[61557],[1...|[[4.21679155E8], ...|\n",
      "| 100058|           [305-2.0]|(65536,[36741],[1...|[[9.9304489E7], [...|\n",
      "| 100062|          [8204-3.0]|(65536,[23240],[1...|[[1.254875851E9],...|\n",
      "| 100081|          [8833-2.0]|(65536,[58195],[1...|[[1.415767196E9],...|\n",
      "| 100083|[1117-4.0, 1195-2...|(65536,[259,857,4...|[[2.00304006E8], ...|\n",
      "| 100087|           [389-3.0]|(65536,[34060],[1...|[[4.98602386E8], ...|\n",
      "| 100091|          [8833-2.5]|(65536,[59593],[1...|[[7.96290547E8], ...|\n",
      "| 100106|[4202-4.5, 4249-5...|(65536,[378,1164,...|[[1.32787764E8], ...|\n",
      "| 100108|[2004-3.5, 2172-1...|(65536,[551,1470,...|[[1.12185413E8], ...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MinHashLSH is a integrated minhashing function on pyspark\n",
    "minhash_movie = MinHashLSH(inputCol=\"vectorized_rates\", outputCol=\"hashed_rates\", numHashTables=64)\n",
    "minhash_model = minhash_movie.fit(feat_vector)\n",
    "signatures = minhash_model.transform(feat_vector)\n",
    "signatures.limit(20).show() #forcing system to calculate it\n",
    "#MinHashLSH already bands and selects the number of rows per band, so no manually selection of bands and rows will be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "393aca18-afd5-4b54-a60e-906714cd32c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|            datasetA|            datasetB|JaccardDistance|\n",
      "+--------------------+--------------------+---------------+\n",
      "|{109151, [7056-3....|{116989, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{146421, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{60585, [7056-3.0...|            0.0|\n",
      "|{109151, [7056-3....|{174655, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{185537, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{186339, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{191815, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{194785, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{203419, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{204282, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{206533, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{227698, [7056-3....|            0.0|\n",
      "|{109151, [7056-3....|{232475, [7056-3....|            0.0|\n",
      "|{116989, [7056-3....|{146421, [7056-3....|            0.0|\n",
      "|{116989, [7056-3....|{60585, [7056-3.0...|            0.0|\n",
      "|{116989, [7056-3....|{174655, [7056-3....|            0.0|\n",
      "|{116989, [7056-3....|{185537, [7056-3....|            0.0|\n",
      "|{116989, [7056-3....|{186339, [7056-3....|            0.0|\n",
      "|{116989, [7056-3....|{191815, [7056-3....|            0.0|\n",
      "|{116989, [7056-3....|{194785, [7056-3....|            0.0|\n",
      "+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#approxSimilarityJoin is a pyspark function to perform a LSH and filter by distance\n",
    "sim_items = minhash_model.approxSimilarityJoin(feat_vector,feat_vector, 0.3, distCol=\"JaccardDistance\")\n",
    "#Avoiding repeated items\n",
    "sim_items = sim_items.filter(F.col(\"datasetA.movieId\") < F.col(\"datasetB.movieId\"))\n",
    "sim_items.limit(20).show()\n",
    "#sim_items.show()\n",
    "#sim_items is a dataframe that has the info on the distance and movies that i might want to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac314198-f156-4ed7-b68e-8ba8828ded63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|movieA|movieB|Similarity|\n",
      "+------+------+----------+\n",
      "|109151|116989|       1.0|\n",
      "|109151|146421|       1.0|\n",
      "|109151| 60585|       1.0|\n",
      "|109151|174655|       1.0|\n",
      "|109151|185537|       1.0|\n",
      "|109151|186339|       1.0|\n",
      "|109151|191815|       1.0|\n",
      "|109151|194785|       1.0|\n",
      "|109151|203419|       1.0|\n",
      "|109151|204282|       1.0|\n",
      "|109151|206533|       1.0|\n",
      "|109151|227698|       1.0|\n",
      "|109151|232475|       1.0|\n",
      "|116989|146421|       1.0|\n",
      "|116989| 60585|       1.0|\n",
      "|116989|174655|       1.0|\n",
      "|116989|185537|       1.0|\n",
      "|116989|186339|       1.0|\n",
      "|116989|191815|       1.0|\n",
      "|116989|194785|       1.0|\n",
      "+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pair_sim is a friendly way to check if X movie is similar to Y movie, and how similar\n",
    "pair_sim = sim_items.withColumn(\"movieA\", F.col(\"datasetA.movieId\")).withColumn(\"movieB\", F.col(\"datasetB.movieId\")).withColumn(\"Similarity\", 1 - F.col(\"JaccardDistance\"))\n",
    "pair_sim = pair_sim.drop(\"datasetA\", \"datasetB\", \"JaccardDistance\")\n",
    "pair_sim.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66528e-b5da-40ff-a048-d21685696502",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\" \n",
    "There is a problem of unnexisting data for example user 1 has the following:\n",
    "+--------+------------------+---------------------+\n",
    "|val_user|actual_rated_movie|actual_rating_by_user|\n",
    "+--------+------------------+---------------------+\n",
    "|       1|              1200|                  3.5|\n",
    "|       1|              1214|                  4.0|\n",
    "|       1|               596|                  4.0|\n",
    "|       1|              7706|                  3.5|\n",
    "\n",
    "But when we try to compare with the predictions\n",
    "+----+------+-------------+----------------+\n",
    "|user| movie|actual_rating|Predicted_rating|\n",
    "+----+------+-------------+----------------+\n",
    "|  22| 74727|          4.0|             4.0|\n",
    "| 123|  6663|          5.0|             4.5|\n",
    "| 305|130452|          4.0|             4.0|\n",
    "+----+------+-------------+----------------+\n",
    "\n",
    "1 doesn't exist at all, in fact for 100 000 samples only users 22, 123, 305 can be compared\n",
    "\n",
    "That happens because during the prediction dataframe creation because we are \"calculating\"\n",
    "the predicted ratings based on similar movies that the USER saw, so if the user didnt see any \n",
    "similar movies to the ones they still haven't seen there is no predictions to be done\n",
    "\n",
    "Now to fill the utility matrix we need a strategy to handle these sorts of cases\n",
    "\n",
    "As a solution i propose to instead calculate a average of the rating of similar movies \n",
    "and by default every user's predicted rating to be that average and to those users who\n",
    "the predicted rating can be calculated by similar seen movies, it is changed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a34b4613-f24b-4ea4-bff6-8e27f6765045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------+\n",
      "|userId|movieId|default_rating|\n",
      "+------+-------+--------------+\n",
      "|  6116|   5229|           3.0|\n",
      "|  6823| 110173|           3.5|\n",
      "|  3083| 133729|           2.5|\n",
      "|  7010| 270578|           2.0|\n",
      "|  5957| 128862|           3.5|\n",
      "|  7644| 238108|           5.0|\n",
      "|  6232| 173445|           3.5|\n",
      "|  6116|   8621|           3.5|\n",
      "|  3951|  59053|           3.5|\n",
      "|  8833|  92204|           3.5|\n",
      "|  7959| 132824|           2.0|\n",
      "|  8833|  39305|           2.5|\n",
      "|  7010| 224024|           2.0|\n",
      "|  8833| 198919|           2.5|\n",
      "|  8833| 151741|           3.0|\n",
      "|  8833| 143065|           2.5|\n",
      "|  9453| 169052|           2.0|\n",
      "|  3884| 132159|           4.5|\n",
      "|  8978| 134093|           3.0|\n",
      "|  8374| 151347|           2.5|\n",
      "+------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#defaulting the predicted values for the ratings (similarity = 0.5 defined above)\n",
    "default_rate_a = data.join(pair_sim, (F.col(\"movieId\") == F.col(\"movieA\")), \"inner\").select(\"userId\",\"movieId\",\"rating\",\"movieB\",\"Similarity\")\n",
    "default_rate_a = default_rate_a.alias(\"a\").join(data.alias(\"b\"), (F.col(\"a.movieB\") == F.col(\"b.movieId\")), \"inner\").select(F.col(\"a.userId\").alias(\"userId\"),F.col(\"a.movieId\").alias(\"movieId\"),F.col(\"a.movieB\").alias(\"movieB\"),F.col(\"Similarity\"), F.col(\"b.rating\").alias(\"ratingB\"))\n",
    "#Calculating the average rating\n",
    "default_rate_a = default_rate_a.groupBy(\"userId\",\"movieId\").agg((F.sum(F.col(\"Similarity\") * F.col(\"ratingB\"))/F.sum(F.col(\"Similarity\"))).alias(\"default_rating\"))\n",
    "#We start by checking what movies in the original data are present on the first column of the similar movies \n",
    "#Then we do the same for movies on the second column, this way no movies are unchecked\n",
    "default_rate_b = data.join(pair_sim, (F.col(\"movieId\") == F.col(\"movieB\")), \"inner\").select(\"userId\",\"movieId\",\"rating\",\"movieA\",\"Similarity\")\n",
    "default_rate_b = default_rate_b.alias(\"a\").join(data.alias(\"b\"), (F.col(\"a.movieA\") == F.col(\"b.movieId\")), \"inner\").select(F.col(\"a.userId\").alias(\"userId\"),F.col(\"a.movieId\").alias(\"movieId\"),F.col(\"a.movieA\").alias(\"movieB\"),F.col(\"Similarity\"), F.col(\"b.rating\").alias(\"ratingB\"))\n",
    "#calculating the average rating\n",
    "default_rate_b = default_rate_b.groupBy(\"userId\",\"movieId\").agg((F.sum(F.col(\"Similarity\") * F.col(\"ratingB\"))/F.sum(F.col(\"Similarity\"))).alias(\"default_rating\"))\n",
    "#Then we unite both dataframes, using distinct to avoid duplicates\n",
    "default_rate = default_rate_a.union(default_rate_b).distinct()\n",
    "default_rate.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d969810-624a-4bb2-ab3c-e971f194c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------+\n",
      "|userId|movieId|default_rating|\n",
      "+------+-------+--------------+\n",
      "|  1001| 100591|           3.0|\n",
      "|  1010| 100591|           3.0|\n",
      "|  1088| 100591|           3.0|\n",
      "|  1093| 100591|           3.0|\n",
      "|  1097| 100591|           3.0|\n",
      "|  1108| 100591|           3.0|\n",
      "|  1148| 100591|           3.0|\n",
      "|  1302| 100591|           3.0|\n",
      "|   131| 100591|           3.0|\n",
      "|  1382| 100591|           3.0|\n",
      "|  1418| 100591|           3.0|\n",
      "|  1430| 100591|           3.0|\n",
      "|  1447| 100591|           3.0|\n",
      "|  1468| 100591|           3.0|\n",
      "|  1469| 100591|           3.0|\n",
      "|  1505| 100591|           3.0|\n",
      "|  1537| 100591|           3.0|\n",
      "|  1615| 100591|           3.0|\n",
      "|  1646| 100591|           3.0|\n",
      "|   165| 100591|           3.0|\n",
      "+------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The idea is to now create a dataframe of users and movies user hasn't rated(seen)\n",
    "#dataframe data has userId, movieId, ratings and userId-ratings(unnecessary for this part)\n",
    "#First we get all the movies available\n",
    "all_movies = data.select(\"movieId\").distinct()\n",
    "all_movies = data.select(\"userId\").distinct().crossJoin(all_movies)\n",
    "#Second we get all the movies rated, by user\n",
    "rated_movies = data.select(\"userId\", \"movieId\")\n",
    "#Lastly we join left_anti to exclude all the rated movies from all movies\n",
    "#not_rated is now a dataframe with all the users and movies they havent rated\n",
    "not_rated = all_movies.join(rated_movies,on=[\"userId\", \"movieId\"], how=\"left_anti\")\n",
    "#Will now integrate the default rating into the unrated movies\n",
    "not_rated = not_rated.alias(\"a\").join(default_rate.alias(\"b\"), F.col(\"a.movieId\") == F.col(\"b.movieId\")).select(F.col(\"a.userId\").alias(\"userId\"), F.col(\"a.movieId\").alias(\"movieId\"), \"default_rating\").distinct()\n",
    "not_rated.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cabc5281-a640-43e5-81df-2ee4a51492bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+--------------+\n",
      "|userId|movieId|movieB|Similarity|default_rating|\n",
      "+------+-------+------+----------+--------------+\n",
      "|  1007| 100591|144618|       1.0|           3.0|\n",
      "|  1070| 100591| 83577|       1.0|           3.0|\n",
      "|   113| 100591|124160|       1.0|           3.0|\n",
      "|   113| 100591| 43626|       1.0|           3.0|\n",
      "|   113| 100591| 66686|       1.0|           3.0|\n",
      "|  1064| 100591| 25783|       1.0|           3.0|\n",
      "|  1064| 100591|  7583|       1.0|           3.0|\n",
      "|  1170| 100591|147028|       1.0|           3.0|\n",
      "|  1170| 100591| 53916|       1.0|           3.0|\n",
      "|  1170| 100591| 80661|       1.0|           3.0|\n",
      "|  1183| 100591| 87595|       1.0|           3.0|\n",
      "|  1257| 100591|175135|       1.0|           3.0|\n",
      "|  1257| 100591| 72344|       1.0|           3.0|\n",
      "|  1224| 100591|108799|       1.0|           3.0|\n",
      "|  1224| 100591| 83577|       1.0|           3.0|\n",
      "|  1306| 100591| 82467|       1.0|           3.0|\n",
      "|  1264| 100591|279530|       1.0|           3.0|\n",
      "|  1264| 100591| 47131|       1.0|           3.0|\n",
      "|  1264| 100591| 94982|       1.0|           3.0|\n",
      "|  1352| 100591|188701|       1.0|           3.0|\n",
      "+------+-------+------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We will now expand the not_rated dataframe to include a prediction of the rating of each user on each unrated film\n",
    "#The idea is to verify similar movies to a unrated movie and see what similar movies\n",
    "#a user rated and ponder a average based on this criteria\n",
    "#dataframe that has the similar movies to the ones the user didnt rate on MovieB column\n",
    "prediction_b = not_rated.join(pair_sim, (F.col(\"movieId\") == F.col(\"movieA\")), \"inner\").drop(\"movieA\").select(\"userId\", \"movieId\",\"movieB\", \"Similarity\", \"default_rating\")\n",
    "#dataframe that has the similar movie to the ones the user didnt rate on MovieA column, calling movieA as movieB for a successfull union\n",
    "prediction_a = not_rated.join(pair_sim, (F.col(\"movieId\") == F.col(\"movieB\")), \"inner\").drop(\"movieB\").select(\"userId\", \"movieId\", F.col(\"movieA\").alias(\"movieB\"), \"Similarity\", \"default_rating\")\n",
    "#Union of both dataframes without repetitions(.distinct())\n",
    "prediction = prediction_b.union(prediction_a).distinct() #.persist(StorageLevel.DISK_ONLY)\n",
    "prediction.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b6fde-e06c-46bd-a632-7ab4917f4664",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "We join prediction with the original data to be able to access it's rating to calculate the predictions(not the defaulted ones)\n",
    "The predicted rating is calculated and saved in a different dataframe because \n",
    "prediction_c is grouped by userId and unrated_movie and that eliminates the default rating that was calculated beforeb\n",
    "The later join between final_prediction and prediction_c is just to integrate the\n",
    "default rating into the predicted rating(named Rate_prediction(with_default))\n",
    "\n",
    "Note that joining the default rating and the predicted rating into the same column will not\n",
    "have any collision as predicted rating is exclusive of the existence of default rating\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d80b0bf-74ad-4d7b-865f-33b9212657d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------------------------+\n",
      "|userId|unrated_movie|Rate_prediction(with_default)|\n",
      "+------+-------------+-----------------------------+\n",
      "|     1|       103576|                          3.5|\n",
      "|     1|       109812|                          4.0|\n",
      "|     1|       111624|                          2.5|\n",
      "|     1|       112735|                          3.5|\n",
      "|     1|       113843|                          4.0|\n",
      "|     1|       116945|                          3.5|\n",
      "|     1|       122270|                          2.5|\n",
      "|     1|       122270|           2.1666666666666665|\n",
      "|     1|       123282|                          2.5|\n",
      "|     1|       135761|                          3.5|\n",
      "|     1|       135785|                          2.0|\n",
      "|     1|       135787|                          4.5|\n",
      "|     1|       139795|                          3.0|\n",
      "|     1|       141137|                          1.5|\n",
      "|     1|       146190|                          2.0|\n",
      "|     1|       146358|                          2.5|\n",
      "|     1|       148056|                          1.5|\n",
      "|     1|       154474|                          4.5|\n",
      "|     1|       155024|                          2.5|\n",
      "|     1|       156777|                          4.5|\n",
      "+------+-------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here we join the prediction with the original data in order to calculate the predicted rating based on similar ratings\n",
    "prediction = prediction.alias(\"a\").join(data.alias(\"b\"), ((F.col(\"movieB\")==F.col(\"b.movieId\")) & (F.col(\"a.userId\") == F.col(\"b.userId\"))), \"left\")\n",
    "#Selecting the only needed columns\n",
    "prediction = prediction.select(F.col(\"a.userId\"), F.col(\"a.movieId\").alias(\"unrated_movie\"), F.col(\"movieB\").alias(\"sim_movie\"),F.col(\"Similarity\"), F.col(\"rating\").alias(\"rating_sim\"), \"default_rating\")\n",
    "#Created a new df to avoid eliminating the default rating\n",
    "prediction_c = prediction.groupBy(\"userId\", \"unrated_movie\").agg((F.sum(F.col(\"Similarity\")*F.col(\"rating_sim\"))/F.sum(F.col(\"Similarity\"))).alias(\"Predicted_rating\")).orderBy(F.asc(\"userId\"))\n",
    "\n",
    "#formatting to the only needed columns\n",
    "final_prediction = prediction.select(\"userId\", \"unrated_movie\", \"default_rating\").distinct()\n",
    "#Join to fit the default rating calculated before into a column\n",
    "final_prediction = final_prediction.alias(\"a\").join(prediction_c.alias(\"b\"), (F.col(\"a.userId\")==F.col(\"b.userId\"))&(F.col(\"a.unrated_movie\") == F.col(\"b.unrated_movie\")), how = \"left\")\n",
    "#Joining the predicted rating and the default rating into the same column\n",
    "final_prediction = final_prediction.withColumn(\"Rate_prediction(with_default)\", F.when(F.col(\"Predicted_rating\").isNotNull(), F.col(\"Predicted_rating\")).otherwise(F.col(\"default_rating\")))\n",
    "#keeping this df in cache because it is a BIG dataframe\n",
    "final_prediction = final_prediction.select(\"a.userId\",\"a.unrated_movie\",\"Rate_prediction(with_default)\").distinct()#.persist(StorageLevel.DISK_ONLY)#.repartition(400)\n",
    "final_prediction.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a296021-2213-4592-a400-cff10e76f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+---------------------+\n",
      "|val_user|actual_rated_movie|actual_rating_by_user|\n",
      "+--------+------------------+---------------------+\n",
      "|       1|              1200|                  3.5|\n",
      "|       1|              1214|                  4.0|\n",
      "|       1|               596|                  4.0|\n",
      "|       1|              7706|                  3.5|\n",
      "|      10|              1148|                  3.5|\n",
      "|      10|              2716|                  3.0|\n",
      "|      10|             34150|                  2.5|\n",
      "|      10|             34162|                  3.0|\n",
      "|      10|             44022|                  3.0|\n",
      "|      10|             59369|                  3.5|\n",
      "|      10|             63113|                  4.0|\n",
      "|      10|             76251|                  3.5|\n",
      "|      10|             79293|                  3.5|\n",
      "|      10|             87222|                  3.5|\n",
      "|      10|             94864|                  4.5|\n",
      "|     100|              1285|                  4.0|\n",
      "|     100|             32587|                  3.0|\n",
      "|    1000|              2316|                  4.5|\n",
      "|    1000|               260|                  3.5|\n",
      "|    1000|              2747|                  2.0|\n",
      "+--------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This is the validation set where we check if some movies were actually rated by the users\n",
    "#The objective of this dataframe is to check if our \"averaged\" rating is correct\n",
    "#Join between the validation data and unrated movies dataframe to check the movies that \n",
    "#were thought to be unrated but were actually rated\n",
    "val_df = data_val.alias(\"a\").join(not_rated.alias(\"b\"), (F.col(\"a.userId\")==F.col(\"b.userId\")) & (F.col(\"a.movieId\") == F.col(\"b.movieId\")), \"left\").select(F.col(\"a.userId\").alias(\"val_user\"), F.col(\"a.movieId\").alias(\"actual_rated_movie\"), F.col(\"a.rating\").alias(\"actual_rating_by_user\")).distinct()\n",
    "val_df.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51b15191-d8cd-4d44-81ec-fb0788dcab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------+-----------------------------+\n",
      "|user| movie|actual_rating|Rate_prediction(with_default)|\n",
      "+----+------+-------------+-----------------------------+\n",
      "|2402| 74582|          2.0|                          3.0|\n",
      "| 265|  7626|          2.0|                          2.5|\n",
      "|2639|101425|          4.0|                          3.5|\n",
      "|3689|154470|          2.5|                          4.5|\n",
      "|9404|173845|          5.0|                          2.5|\n",
      "|4190| 26746|          2.0|                          3.5|\n",
      "|4733|  4669|          3.0|                          3.0|\n",
      "|4934|  6854|          4.0|                          3.0|\n",
      "|6386|171461|          1.5|                          3.0|\n",
      "|6416|  2509|          3.5|                          1.0|\n",
      "|9521|  7583|          3.0|                          3.0|\n",
      "|2935| 92923|          2.0|                          1.0|\n",
      "| 301|256075|          3.0|                          3.5|\n",
      "|8833| 87878|          3.0|                          4.0|\n",
      "|9401|275173|          3.5|                          1.0|\n",
      "|2010|  2008|          2.0|                          3.0|\n",
      "| 203| 95615|          5.0|                          4.5|\n",
      "|5999| 54196|          4.5|                          4.5|\n",
      "|8816|205515|          3.5|                          2.0|\n",
      "|2568|115540|          4.0|                          3.5|\n",
      "+----+------+-------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here we create a dataframe that displays the ratings the movies that were predicted compared to the same movies\n",
    "#left in the actual validation data\n",
    "pred_actual_df = val_df.join(final_prediction, (F.col(\"val_user\") == F.col(\"userId\")) & (F.col(\"actual_rated_movie\") == F.col(\"unrated_movie\")), how=\"inner\") #inner\n",
    "pred_actual_df = pred_actual_df.select(F.col(\"val_user\").alias(\"user\"), F.col(\"actual_rated_movie\").alias(\"movie\"), F.col(\"actual_rating_by_user\").alias(\"actual_rating\"), F.col(\"Rate_prediction(with_default)\")).distinct()\n",
    "pred_actual_df.limit(20).show()\n",
    "#pred_actual_df.write.option(\"header\", True).csv(\"pred_vs_actual1M.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61eea9-d9c9-4d0f-ab81-ed35ce620ed8",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "I was getting an extremely small dataframe for the predicted rating vs actual ratings such as this one\n",
    "+----+------+-------------+----------------+\n",
    "|user| movie|actual_rating|Predicted_rating| ###This dataframe was for 10K lines\n",
    "+----+------+-------------+----------------+\n",
    "|  22| 74727|          4.0|             4.0|\n",
    "| 123|  6663|          5.0|             4.5|\n",
    "| 305|130452|          4.0|             4.0|\n",
    "+----+------+-------------+----------------+\n",
    "So due to these small dataframes it was decided to include the default rating system\n",
    "It's not available but even with the default rating included the dataframes were still small\n",
    "Example: (prints counting the number of elements of a given dataframe)\n",
    "validation dataframe size 10062\n",
    "pred_actual_df count 365\n",
    "\n",
    "Then after getting pred_actual_df to be the join between val_df and final_prediction to be \"left\"\n",
    "the following  dataframe was the result\n",
    "+----+------+-------------+-----------------------------+\n",
    "|user| movie|actual_rating|Rate_prediction(with_default)|\n",
    "+----+------+-------------+-----------------------------+\n",
    "|  33| 48516|          4.5|                         NULL|\n",
    "|  50|122900|          1.5|                         NULL|\n",
    "|  53|  1206|          4.5|                         NULL|\n",
    "|  11| 91529|          4.5|                         NULL|\n",
    "|  82|  1093|          2.0|                         NULL|\n",
    "|  91|   500|          3.0|                         NULL|\n",
    "|  82|   968|          5.0|                          5.0|\n",
    "\n",
    "We have plenty of movies rate_prediction as NULL, that requires the question even with the \n",
    "default_rating why does the rate prediction keep being null?\n",
    "\n",
    "One of the hypothesis is that these movies only appear in the validation data \n",
    "hence they cannot be rated  \n",
    "\n",
    "this dataframe below, movie_check, is to check the hypothesis above\n",
    "If the following dataframe prints empty that would mean that there is no movie whose\n",
    "Rate_prediction(with_default) is NULL present in the original dataframe named datab\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a6b38bd-de33-4344-8a76-dcac912a8909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering the pred_actual_df by Rate_prediction being Null\n",
    "movie_check = pred_actual_df.filter(F.col(\"Rate_prediction(with_default)\").isNull())\n",
    "#joining with data to check if movie that has a Null predicted_rating is available in the original data\n",
    "movie_check = movie_check.join(data, (F.col(\"movie\") == F.col(\"movieId\")) & (F.col(\"user\")==F.col(\"userId\")), \"inner\")\n",
    "movie_check.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84218fe4-eaae-4d34-9901-3f9daa8705e4",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "Empty dataframe, this means that all the Null ratings are due to movies not being\n",
    "available in the original data\n",
    "+----+-----+-------------+-----------------------------+------+-------+------+-------------+\n",
    "|user|movie|actual_rating|Rate_prediction(with_default)|userId|movieId|rating|userId-rating|\n",
    "+----+-----+-------------+-----------------------------+------+-------+------+-------------+\n",
    "+----+-----+-------------+-----------------------------+------+-------+------+-------------+\n",
    "\n",
    "To avoid big calculations ill keep the dataframe that compares actual vs predicted rating as\n",
    "\"inner\" join so this dataframe will always appear empty\n",
    "\n",
    "Also with the adding of more data the number of unavailable movies on the \"training\" data\n",
    "compared to the validation data are expected to get relatively lower and lower because\n",
    "this dataset features 86k movies, and it is expected with the increasing of the data more users\n",
    "rating more movies.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403c13c-106c-40dd-9408-1e6d3bc60f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|SQRT(avg(POWER((actual_rating - Rate_prediction(with_default)), 2)))|\n",
      "+--------------------------------------------------------------------+\n",
      "|                                                   1.326080173877682|\n",
      "+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. RMSE (Root Mean Square Error)\n",
    "rmse = pred_actual_df.select(\n",
    "    F.sqrt(F.avg(F.pow(F.col(\"actual_rating\") - F.col(\"Rate_prediction(with_default)\"), 2))).alias(\"RMSE\"))\n",
    "rmse.show()\n",
    "\n",
    "# 2. MAE (Mean Absolute Error)\n",
    "mae = pred_actual_df.select(\n",
    "    F.avg(F.abs(F.col(\"actual_rating\") - F.col(\"Rate_prediction(with_default)\"))).alias(\"MAE\"))\n",
    "mae.show()\n",
    "\n",
    "# 4. MAPE (Mean Absolute Percentage Error)\n",
    "def calculate_ape(actual, predicted):\n",
    "    return F.abs((actual - predicted) / actual) * 100\n",
    "\n",
    "mape = pred_actual_df.withColumn(\"ape\", \n",
    "    calculate_ape(F.col(\"actual_rating\"), F.col(\"Rate_prediction(with_default)\"))) \\\n",
    "    .select(F.avg(\"ape\"))\n",
    "mape.show()\n",
    "\n",
    "# 5. Acurácia para classificações inteiras (se aplicável)\n",
    "accuracy = pred_actual_df.withColumn(\"correct\", \n",
    "    when(F.round(F.col(\"Rate_prediction(with_default)\")) == F.col(\"actual_rating\"), 1).otherwise(0)) \\\n",
    "    .select(F.avg(\"correct\"))\n",
    "accuracy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a3d10-fa08-43fe-9a1c-dcb86208d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3.5\n",
    "\n",
    "precision = pred_actual_df.withColumn(\"relevant_pred\", \n",
    "    col(\"Rate_prediction(with_default)\") >= threshold) \\\n",
    "    .withColumn(\"relevant_actual\", \n",
    "    col(\"actual_rating\") >= threshold) \\\n",
    "    .withColumn(\"true_positive\", \n",
    "    (col(\"relevant_pred\") & col(\"relevant_actual\")).cast(\"int\")) \\\n",
    "    .withColumn(\"false_positive\", \n",
    "    (col(\"relevant_pred\") & ~col(\"relevant_actual\")).cast(\"int\")) \\\n",
    "    .agg(\n",
    "        F.sum(\"true_positive\").alias(\"tp\"),\n",
    "        F.sum(\"false_positive\").alias(\"fp\")\n",
    "    ).withColumn(\"precision\", col(\"tp\") / (col(\"tp\") + col(\"fp\"))))\n",
    "precision.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2f393a2-4b9e-4166-9476-4bf4aad7f688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10306\n",
      "10306\n",
      "prediction size at union 9115745967\n",
      "prediction size after left join with data, should be equal to the previous number 9115745967\n",
      "validation data size 200092\n",
      "validation dataframe size 200092\n",
      "pred_actual_df count 740\n",
      "How many NULL ratings 0\n",
      "movie_check count 0\n"
     ]
    }
   ],
   "source": [
    "###This section is just to make sure the dimensions are correct when moving from dataframe to dataframe\n",
    "#as it is easy to filter data unwantingly\n",
    "\"\"\"\n",
    "3966 movies that user 1 hasn't rated for 10k obs\n",
    "3966 movies not rated by user 1\n",
    "checks out, dimensions are correct for the unrated_movies for user 1, will assume correctness for all other users\n",
    "\"\"\"\n",
    "print(not_rated.filter(F.col(\"userId\")==1).select(\"movieId\").distinct().count())\n",
    "print(prediction.filter(F.col(\"userId\")==1).select(\"unrated_movie\").distinct().count())\n",
    "\n",
    "\"\"\"\n",
    "prediction size at union 134925998\n",
    "prediction size after left join with data, should be equal to the previous number 134925998\n",
    "Checks out, dimensions are the same\n",
    "\"\"\"\n",
    "print(f\"prediction size at union {prediction.count()}\")\n",
    "print(f\"prediction size after left join with data, should be equal to the previous number {prediction.count()}\")\n",
    "\n",
    "#print(f\"prediction_c size after grouping {prediction_c.count()}\")\n",
    "#print(f\"final prediction dataframe size {final_prediction.count()}\")\n",
    "\n",
    "\"\"\"\n",
    "validation data size 10062\n",
    "validation dataframe size 10062\n",
    "checks out\n",
    "\"\"\"\n",
    "print(f\"validation data size {data_val.count()}\")\n",
    "print(f\"validation dataframe size {val_df.count()}\")\n",
    "\n",
    "\"\"\"\n",
    "pred_actual_df count 10072 (intersection between unrated movies and validation data)\n",
    "How many NULL ratings 9707 ()\n",
    "movie_check count 0\n",
    "As stated earlier this is expected when pred_actual_df is created using a left join\n",
    "\"\"\"\n",
    "print(f\"pred_actual_df count {pred_actual_df.count()}\")\n",
    "print(f\"How many NULL ratings {pred_actual_df.filter(F.col('Rate_prediction(with_default)').isNull()).count()}\")\n",
    "print(f\"movie_check count {movie_check.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
